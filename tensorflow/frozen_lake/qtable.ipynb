{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.action_size = env.action_space.n\n",
    "        \n",
    "    def random_action(self, state):\n",
    "        action = random.choice(range(self.action_size))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Update: $Q(s_{t}, a_{t}) = r_{t+1} + \\gamma max(Qs_{t+1})$ <br>\n",
    "Q Learn: $Q(s,a) \\leftarrow Q(s,a) + \\alpha(target - Q(s,a)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, env, discount_rate=0.97, learning_rate=0.65):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.exploration_rate = .99\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):  \n",
    "        self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "     \n",
    "    def get_action(self, state):\n",
    "        possible_actions = self.q_table[state] \n",
    "        exploit = np.argmax(possible_actions)  \n",
    "        explore = super().random_action(state)\n",
    "        return explore if random.random() < self.exploration_rate else exploit\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward, done = experience\n",
    "        q_next = np.zeros([self.action_size]) if done else self.q_table[next_state]\n",
    "        \n",
    "        q_target = reward + self.discount_rate * np.max(q_next)\n",
    "        q_update = q_target - self.q_table[state, action] \n",
    "        self.q_table[state, action] = self.q_table[state, action] + self.learning_rate * (q_target - self.q_table[state, action])\n",
    "        \n",
    "        if done:\n",
    "            self.exploration_rate = self.exploration_rate * .99\n",
    "\n",
    "total_reward = 0\n",
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 15, Action: 3\n",
      "Episode:99, Total reward:66.0, eps: 0.017771047742294682\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[1.08587747e-01 1.04904330e-01 1.26793140e-01 9.89639228e-02]\n",
      " [2.03765227e-03 2.92810547e-02 3.36663970e-02 1.08347173e-01]\n",
      " [7.26457333e-02 6.48497144e-02 5.58298691e-02 8.56593283e-02]\n",
      " [1.72501795e-02 2.92711621e-02 4.91821266e-03 7.00915388e-02]\n",
      " [1.83281290e-01 1.33442497e-01 6.06294069e-02 7.52311886e-02]\n",
      " [5.03771945e-05 1.18980453e-05 3.96564600e-05 6.93966483e-05]\n",
      " [1.03554716e-02 1.73349438e-06 7.00790717e-02 1.26454125e-02]\n",
      " [2.71989523e-05 8.12533337e-05 1.47618955e-05 7.55793198e-05]\n",
      " [6.37660648e-02 1.20847864e-01 2.98056892e-02 3.25246230e-01]\n",
      " [3.70788375e-02 3.90755476e-01 1.96552453e-02 2.33259026e-02]\n",
      " [5.64498154e-01 3.93844730e-02 7.51683284e-02 1.73298040e-02]\n",
      " [6.63392192e-05 1.50279353e-05 3.48091677e-05 6.19481277e-05]\n",
      " [3.88999906e-05 9.52140135e-05 9.61827232e-05 1.06183791e-05]\n",
      " [1.87426054e-01 1.76388968e-01 6.95528962e-01 1.56362836e-01]\n",
      " [5.62526455e-01 5.17340516e-01 4.00726770e-01 9.59950456e-01]\n",
      " [7.73637759e-05 4.28187810e-05 2.31222202e-05 8.04316529e-06]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.train((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"State: {state}, Action: {action}\")\n",
    "        print(f\"Episode:{episode}, Total reward:{total_reward}, eps: {agent.exploration_rate}\")\n",
    "        env.render()\n",
    "        print(agent.q_table)\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
