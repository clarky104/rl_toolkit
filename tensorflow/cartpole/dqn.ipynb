{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = .995\n",
    "EXPLORATION_MIN = 0.01\n",
    "GAMMA = 0.90\n",
    "ALPHA = 0.002\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: ####\n",
    ">Type: Box(4) <br>\n",
    ">Num &emsp;&emsp; Observation &emsp;&emsp;&emsp;&emsp; Min &emsp;&emsp;&emsp;&emsp; Max <br>\n",
    ">0   &emsp;&emsp;&emsp;&ensp; Cart Position &emsp;&emsp;&emsp;&emsp; -4.8 &emsp;&emsp;&emsp;&emsp; 4.8 <br>\n",
    ">1   &emsp;&emsp;&emsp;&ensp; Cart Velocity &emsp;&emsp;&emsp;&emsp; -Inf &emsp;&emsp;&emsp;&emsp;&nbsp; Inf <br>\n",
    ">2   &emsp;&emsp;&emsp;&ensp; Pole Angle &emsp;&emsp;&emsp;&emsp;&emsp; -24° &emsp;&emsp;&emsp;&ensp;&nbsp; 24° <br>\n",
    ">3  &emsp;&emsp;&emsp;&ensp; Pole Velocity At Tip &emsp;&nbsp; -Inf &emsp;&emsp;&emsp;&emsp;&nbsp; Inf <br>\n",
    "\n",
    "#### Action: ####\n",
    ">Type: Discrete(2) <br>\n",
    ">Num &emsp;&emsp; Action <br>\n",
    ">0   &emsp;&emsp;&emsp;&ensp; Push cart to the left <br>\n",
    ">1   &emsp;&emsp;&emsp;&ensp; Push cart to the right\n",
    "\n",
    "#### Reward: ####\n",
    ">Reward: +1 for every timestep in upward position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q Update:** $Q(s_{t}, a_{t}) = r_{t+1} + \\gamma max(Qs_{t+1})$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EPSILON\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(16, input_shape=(observation_space,), activation='relu'))\n",
    "        self.model.add(Dense(32, input_shape=(observation_space,), activation='relu'))\n",
    "        self.model.add(Dense(self.action_space, activation='softmax'))\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=ALPHA))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "    \n",
    "    def save_weights(self, filepath):\n",
    "        self.model.save_weights(filepath)\n",
    "        \n",
    "    def random_action(self, state):\n",
    "        return random.randrange(self.action_space)\n",
    "    \n",
    "    def optimal_action(self, state):\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return self.random_action(state)\n",
    "        return self.optimal_action(state)\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            batch = random.sample(self.memory, BATCH_SIZE)\n",
    "            for state, action, reward, next_state, done in batch:\n",
    "                q_update = reward\n",
    "                if not done:\n",
    "                    q_update = reward + GAMMA * np.amax(self.model.predict(next_state))\n",
    "                q_values = self.model.predict(state)\n",
    "                q_values[0][action] = q_update\n",
    "                self.model.fit(state, q_values, verbose=0)\n",
    "                \n",
    "            self.exploration_rate *= EPSILON_DECAY\n",
    "            self.exploration_rate = max(self.exploration_rate, EXPLORATION_MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, environment_name):\n",
    "        self.env = gym.make(environment_name)\n",
    "        self.observation_space = self.env.observation_space.shape[0]\n",
    "        self.action_space = self.env.action_space.n\n",
    "        self.agent = Agent(self.observation_space, self.action_space)\n",
    "                \n",
    "    def learn_policy(self, state):\n",
    "        action = self.agent.choose_action(state)\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, self.observation_space])\n",
    "        self.agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        self.agent.experience_replay()\n",
    "        return state, reward, done\n",
    "        \n",
    "    def act_random(self, state):\n",
    "        self.env.render()\n",
    "        action = self.agent.random_action(state)\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, self.observation_space])\n",
    "        state = next_state\n",
    "        return state, reward, done\n",
    "            \n",
    "    def act_optimal(self, state):\n",
    "        self.env.render()\n",
    "        action = self.agent.optimal_action(state)\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, self.observation_space])\n",
    "        state = next_state\n",
    "        return state, reward, done\n",
    "                         \n",
    "    def evaluation(self, function, save_weights=False, load_weights=False, filepath=''):\n",
    "        reward_data = []\n",
    "        if load_weights:\n",
    "            self.agent.load_weights(filepath)\n",
    "            \n",
    "        for episode in range(100):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.observation_space])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                state, reward, done = function(state)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            reward_data.append(total_reward)\n",
    "                \n",
    "        self.env.close()\n",
    "        if save_weights:\n",
    "            self.agent.save_weights(filepath)\n",
    "        return reward_data\n",
    "    \n",
    "    def plot(self, optimal_action, random_action):\n",
    "        plt.plot(optimal_action, label='Optimal Action')\n",
    "        plt.plot(random_action, label='Random Action')\n",
    "        plt.title('Total Cumulative Reward')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    environment = Environment('CartPole-v1')\n",
    "    random_action = environment.evaluation(environment.act_random)\n",
    "    environment.evaluation(environment.learn_policy, save_weights=True, filepath='cartpole_dqn.h5')\n",
    "    optimal_action = environment.evaluation(environment.act_optimal)\n",
    "    environment.plot(optimal_action, random_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
